{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7_hx8SJJR4-"
   },
   "source": [
    "# Assignment 2 - Recurrent Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AiWTVDf7cZ2"
   },
   "source": [
    "## Programming (Full points: 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, our goal is to use PyTorch to implement Recurrent Neural Networks (RNN) for sentiment analysis task. Sentiment analysis is to classify sentences (input) into certain sentiments (output labels), which includes positive, negative and neutral.\n",
    "\n",
    "We will use a benckmark dataset, SST, for this assignment.\n",
    "* we download the SST dataset from torchtext package, and do some preprocessing to build vocabulary and split the dataset into training/validation/test sets. You don't need to modify the code in this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "# Set device for PyTorch operations\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# load data splits\n",
    "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL)\n",
    "\n",
    "# build dictionary\n",
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# hyperparameters\n",
    "vocab_size = len(TEXT.vocab)\n",
    "label_size = len(LABEL.vocab)\n",
    "padding_idx = TEXT.vocab.stoi['<pad>']\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "\n",
    "# build iterators\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* define the training and evaluation function in the cell below.\n",
    "### (25 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get text and labels, move to device\n",
    "        text = batch.text.to(device)\n",
    "        labels = batch.label.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Get text and labels, move to device\n",
    "            text = batch.text.to(device)\n",
    "            labels = batch.label.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(text)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted_labels = predictions.argmax(dim=1)\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    accuracy = correct_predictions / total_samples\n",
    "    avg_loss = epoch_loss / len(iterator)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* build a RNN model for sentiment analysis in the cell below.\n",
    "We have provided several hyperparameters we needed for building the model, including vocabulary size (vocab_size), the word embedding dimension (embedding_dim), the hidden layer dimension (hidden_dim), the number of layers (num_layers) and the number of sentence labels (label_size). Please fill in the missing codes, and implement a RNN model.\n",
    "### (40 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.label_size = label_size\n",
    "        self.num_layers = 1\n",
    "\n",
    "        # add the layers required for sentiment analysis.\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.label_size)\n",
    "\n",
    "    def zero_state(self, batch_size):\n",
    "        # return initial hidden state on the correct device\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text: [batch_size, seq_len]\n",
    "        embedding = self.embedding(text)\n",
    "        hidden = self.zero_state(text.size(0))\n",
    "        outputs, hidden = self.rnn(embedding, hidden)\n",
    "        final_hidden = hidden[-1]\n",
    "        logits = self.fc(final_hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train the model and compute the accuracy in the cell below.\n",
    "### (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | train loss: 1.0555 | val loss: 1.1291 | val acc: 0.3933\n",
      "Epoch 2/5 | train loss: 1.0482 | val loss: 1.0757 | val acc: 0.3960\n",
      "Epoch 3/5 | train loss: 1.0484 | val loss: 1.1943 | val acc: 0.2643\n",
      "Epoch 4/5 | train loss: 1.0460 | val loss: 1.2473 | val acc: 0.2988\n",
      "Epoch 5/5 | train loss: 1.0463 | val loss: 1.1192 | val acc: 0.3170\n",
      "Test loss: 1.0917 | Test acc: 0.4059\n"
     ]
    }
   ],
   "source": [
    "# train baseline RNN model and report accuracies\n",
    "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 5\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_iter, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_iter, criterion)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f} | val acc: {val_acc:.4f}\")\n",
    "\n",
    "# load best validation model before testing\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
    "print(f\"Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* try to train a model with better accuracy in the cell below. For example, you can use different optimizers such as SGD and Adam. You can also compare different hyperparameters and model size.\n",
    "### (15 points), to obtain FULL point in this problem, the accuracy needs to be higher than 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35 | train loss: 1.0262 | val loss: 0.9497 | val acc: 0.5786 | lr: 0.001000\n",
      "Epoch 2/35 | train loss: 0.8801 | val loss: 0.9375 | val acc: 0.5967 | lr: 0.001000\n",
      "Epoch 3/35 | train loss: 0.7137 | val loss: 0.9295 | val acc: 0.6394 | lr: 0.001000\n",
      "Epoch 4/35 | train loss: 0.5616 | val loss: 1.0763 | val acc: 0.6213 | lr: 0.001000\n",
      "Epoch 5/35 | train loss: 0.4412 | val loss: 1.1159 | val acc: 0.6058 | lr: 0.001000\n",
      "Epoch 6/35 | train loss: 0.3333 | val loss: 1.4733 | val acc: 0.6067 | lr: 0.001000\n",
      "Epoch 7/35 | train loss: 0.2552 | val loss: 1.7194 | val acc: 0.5686 | lr: 0.001000\n",
      "Epoch 8/35 | train loss: 0.1847 | val loss: 2.0243 | val acc: 0.5995 | lr: 0.001000\n",
      "Epoch 9/35 | train loss: 0.1523 | val loss: 2.0363 | val acc: 0.5985 | lr: 0.000500\n",
      "Epoch 10/35 | train loss: 0.0826 | val loss: 2.5871 | val acc: 0.5813 | lr: 0.000500\n",
      "Epoch 11/35 | train loss: 0.0424 | val loss: 2.6602 | val acc: 0.5904 | lr: 0.000500\n",
      "Epoch 12/35 | train loss: 0.0370 | val loss: 2.8595 | val acc: 0.5949 | lr: 0.000500\n",
      "Epoch 13/35 | train loss: 0.0287 | val loss: 2.9315 | val acc: 0.5886 | lr: 0.000500\n",
      "Early stopping at epoch 13\n",
      "FINE-TUNED MODEL RESULTS:\n",
      "\n",
      "Final accuracy: 65.25% (target: >70%)\n"
     ]
    }
   ],
   "source": [
    "class AttentiveBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.embed_drop = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        lstm_out_dim = hidden_dim * 2\n",
    "        self.attn = nn.Linear(lstm_out_dim, 1, bias=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        # FC input is 3x: last_hidden + attention + max_pooling\n",
    "        self.fc1 = nn.Linear(lstm_out_dim * 3, lstm_out_dim)\n",
    "        self.fc2 = nn.Linear(lstm_out_dim, label_size)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embed_drop(self.embedding(text))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Get last hidden state (forward and backward)\n",
    "        last_hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)  # [batch, hidden*2]\n",
    "        \n",
    "        # Attention pooling\n",
    "        attn_weights = torch.softmax(self.attn(outputs), dim=1)\n",
    "        attn_rep = torch.sum(attn_weights * outputs, dim=1)\n",
    "        \n",
    "        # Max pooling\n",
    "        max_rep = torch.max(outputs, dim=1)[0]\n",
    "        \n",
    "        # Combine all three: last hidden + attention + max pooling\n",
    "        rep = torch.cat([last_hidden, attn_rep, max_rep], dim=1)\n",
    "        \n",
    "        # Deeper FC layers\n",
    "        x = self.drop(torch.relu(self.fc1(rep)))\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "def train_with_clipping(model, iterator, optimizer, criterion, clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text = batch.text.to(device)\n",
    "        labels = batch.label.to(device)\n",
    "        preds = model(text)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(iterator)\n",
    "\n",
    "# Standard cross-entropy loss (removed label smoothing for better learning)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tuned model: Aggressive improvements to push over 70%\n",
    "# Larger model with triple combination: last hidden + attention + max pooling\n",
    "model_finetuned = AttentiveBiLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=500,  # Increased\n",
    "    hidden_dim=400,      # Increased  \n",
    "    label_size=label_size,\n",
    "    padding_idx=padding_idx,\n",
    "    num_layers=4,        # 4 layers for more capacity\n",
    "    dropout=0.3,         # Lower dropout for more learning\n",
    ").to(device)\n",
    "\n",
    "# Adam optimizer with optimized learning rate\n",
    "optimizer_ft = optim.Adam(model_finetuned.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "# ReduceLROnPlateau scheduler\n",
    "scheduler_ft = optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='max', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "num_epochs_ft = 35  # More epochs\n",
    "patience_ft = 10     # More patience to allow model to converge\n",
    "best_val_acc_ft = 0.0\n",
    "best_state_ft = None\n",
    "patience_counter_ft = 0\n",
    "\n",
    "for epoch in range(num_epochs_ft):\n",
    "    train_loss = train_with_clipping(model_finetuned, train_iter, optimizer_ft, criterion, clip=1.0)\n",
    "    val_loss, val_acc = evaluate(model_finetuned, val_iter, criterion)\n",
    "    scheduler_ft.step(val_acc)  # ReduceLROnPlateau needs validation metric\n",
    "    \n",
    "    if val_acc > best_val_acc_ft:\n",
    "        best_val_acc_ft = val_acc\n",
    "        best_state_ft = copy.deepcopy(model_finetuned.state_dict())\n",
    "        patience_counter_ft = 0\n",
    "    else:\n",
    "        patience_counter_ft += 1\n",
    "    \n",
    "    current_lr = optimizer_ft.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_ft} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f} | val acc: {val_acc:.4f} | lr: {current_lr:.6f}\")\n",
    "    \n",
    "    if patience_counter_ft >= patience_ft:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "if best_state_ft is not None:\n",
    "    model_finetuned.load_state_dict(best_state_ft)\n",
    "\n",
    "test_loss_ft, test_acc_ft = evaluate(model_finetuned, test_iter, criterion)\n",
    "print(\"FINE-TUNED MODEL RESULTS:\")\n",
    "print(f\"\\nFinal accuracy: {test_acc_ft*100:.2f}% (target: >70%)\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
