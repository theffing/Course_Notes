{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7_hx8SJJR4-"
   },
   "source": [
    "# Assignment 2 - Recurrent Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AiWTVDf7cZ2"
   },
   "source": [
    "## Programming (Full points: 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, our goal is to use PyTorch to implement Recurrent Neural Networks (RNN) for sentiment analysis task. Sentiment analysis is to classify sentences (input) into certain sentiments (output labels), which includes positive, negative and neutral.\n",
    "\n",
    "We will use a benckmark dataset, SST, for this assignment.\n",
    "* we download the SST dataset from torchtext package, and do some preprocessing to build vocabulary and split the dataset into training/validation/test sets. You don't need to modify the code in this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "# Set device for PyTorch operations\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# load data splits\n",
    "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL)\n",
    "\n",
    "# build dictionary\n",
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# hyperparameters\n",
    "vocab_size = len(TEXT.vocab)\n",
    "label_size = len(LABEL.vocab)\n",
    "padding_idx = TEXT.vocab.stoi['<pad>']\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "\n",
    "# build iterators\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* define the training and evaluation function in the cell below.\n",
    "### (25 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get text and labels, move to device\n",
    "        text = batch.text.to(device)\n",
    "        labels = batch.label.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Get text and labels, move to device\n",
    "            text = batch.text.to(device)\n",
    "            labels = batch.label.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(text)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted_labels = predictions.argmax(dim=1)\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    accuracy = correct_predictions / total_samples\n",
    "    avg_loss = epoch_loss / len(iterator)\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* build a RNN model for sentiment analysis in the cell below.\n",
    "We have provided several hyperparameters we needed for building the model, including vocabulary size (vocab_size), the word embedding dimension (embedding_dim), the hidden layer dimension (hidden_dim), the number of layers (num_layers) and the number of sentence labels (label_size). Please fill in the missing codes, and implement a RNN model.\n",
    "### (40 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.label_size = label_size\n",
    "        self.num_layers = 1\n",
    "\n",
    "        # add the layers required for sentiment analysis.\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.label_size)\n",
    "\n",
    "    def zero_state(self, batch_size):\n",
    "        # return initial hidden state on the correct device\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text: [batch_size, seq_len]\n",
    "        embedding = self.embedding(text)\n",
    "        hidden = self.zero_state(text.size(0))\n",
    "        outputs, hidden = self.rnn(embedding, hidden)\n",
    "        final_hidden = hidden[-1]\n",
    "        logits = self.fc(final_hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train the model and compute the accuracy in the cell below.\n",
    "### (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | train loss: 1.0545 | val loss: 1.0457 | val acc: 0.5395\n",
      "Epoch 2/5 | train loss: 1.0475 | val loss: 1.1039 | val acc: 0.4069\n",
      "Epoch 3/5 | train loss: 1.0461 | val loss: 1.1084 | val acc: 0.3797\n",
      "Epoch 4/5 | train loss: 1.0454 | val loss: 1.0516 | val acc: 0.5268\n",
      "Epoch 5/5 | train loss: 1.0446 | val loss: 1.2143 | val acc: 0.3097\n",
      "Test loss: 1.0564 | Test acc: 0.5077\n"
     ]
    }
   ],
   "source": [
    "# train baseline RNN model and report accuracies\n",
    "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 5\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_iter, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_iter, criterion)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f} | val acc: {val_acc:.4f}\")\n",
    "\n",
    "# load best validation model before testing\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
    "print(f\"Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* try to train a model with better accuracy in the cell below. For example, you can use different optimizers such as SGD and Adam. You can also compare different hyperparameters and model size.\n",
    "### (15 points), to obtain FULL point in this problem, the accuracy needs to be higher than 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
